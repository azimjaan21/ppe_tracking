import cv2
import time
from ultralytics import YOLO
import torch


import numpy as np

# Function to generate unique color for each track ID
def generate_color(track_id):
    np.random.seed(int(track_id))  # Seed with track_id to ensure unique color
    return tuple(np.random.randint(0, 255, 3).tolist())  # Random color (R, G, B)

# Load YOLOv8m model and move it to CUDA (GPU)
model = YOLO('yolov8m.pt')
model.cuda()  # Move the model to CUDA (GPU)

# Get CUDA device name
device_name = torch.cuda.get_device_name(0)

# Open video capture (use your camera or video file)
cap = cv2.VideoCapture(0)  # 0 for the default webcam, or replace with video file path

# Create a named window for full screen
cv2.namedWindow("Tracking", cv2.WINDOW_NORMAL)
cv2.setWindowProperty("Tracking", cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)

# FPS calculation variables
prev_frame_time = 0
confidence_threshold = 0.5  # Confidence score threshold for detecting persons

# Font settings for text
font = cv2.FONT_HERSHEY_SIMPLEX
font_scale = 0.7
thickness = 1
bg_padding = 5  # Padding around the text

while True:
    ret, frame = cap.read()
    if not ret:
        break

    # Get the current time to calculate FPS
    new_frame_time = time.time()
    fps = 1 / (new_frame_time - prev_frame_time)
    prev_frame_time = new_frame_time

    # Convert frame to CUDA if using GPU
    frame_cuda = frame
    # Perform YOLOv8 detection and tracking
    results = model.track(frame_cuda, classes=[0])

    # Iterate over the results (list of frames)
    for result in results:
        for box in result.boxes:
            # The box contains: x1, y1, x2, y2, confidence, class_id, and track_id
            if box.cls == 0:  # class 0 is "person"
                # Get the confidence score
                conf_score = box.conf[0].item()  # Get confidence score for the person

                if conf_score >= confidence_threshold:  # Only process if confidence >= 0.5
                    # Access the coordinates as a tensor and extract the values
                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()  # Convert tensor to numpy array
                    track_id = box.id  # Track ID for the object

                    # Draw the bounding box and the label "person" on the image
                    # Generate a unique color for each track_id
                    color = generate_color(track_id)

                    # Draw the bounding box with the unique color
                    cv2.rectangle(result.orig_img, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)

                    cv2.putText(result.orig_img, f"Person ID: {int(track_id.item())}", (int(x1), int(y1)-10),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)
                    # Optionally display the confidence score
                    cv2.putText(result.orig_img, f"Conf: {conf_score:.2f}", (int(x1), int(y2) + 20),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)

    # Display FPS and device name in the top-left corner
    fps_text = f"FPS: {fps:.2f}"
    device_text = f"Device: {device_name}"

    # Calculate text sizes
    fps_size = cv2.getTextSize(fps_text, font, font_scale, thickness)[0]
    device_size = cv2.getTextSize(device_text, font, font_scale, thickness)[0]

    # Define positions for the text
    fps_pos = (10, fps_size[1] + 10)
    device_pos = (10, fps_pos[1] + device_size[1] + 5)

    # Draw black rectangles for text background
    cv2.rectangle(result.orig_img, 
                  (fps_pos[0] - bg_padding, fps_pos[1] - fps_size[1] - bg_padding),
                  (fps_pos[0] + fps_size[0] + bg_padding, fps_pos[1] + bg_padding),
                  (0, 0, 0), -1)
    cv2.rectangle(result.orig_img,
                  (device_pos[0] - bg_padding, device_pos[1] - device_size[1] - bg_padding),
                  (device_pos[0] + device_size[0] + bg_padding, device_pos[1] + bg_padding),
                  (0, 0, 0), -1)

    # Draw text on top of the black rectangles
    cv2.putText(result.orig_img, fps_text, fps_pos, font, font_scale, (255, 255, 0), thickness)
    cv2.putText(result.orig_img, device_text, device_pos, font, font_scale, (255, 0, 255), thickness)

    # Display the result
    cv2.imshow("Tracking", result.orig_img)

    # Break the loop on pressing 'q'
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release resources
cap.release()
cv2.destroyAllWindows()
